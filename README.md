This is an assignment for the master programme of Applied Data Science provided by Canterbury University.

It aims to play around the terabytes, gigabytes data in hadoop by pyspark.

The GHCN dataset includes a 255 year summary of global weather information, up to 1TB. The weather daily includes 2,624,027,105 rows of observed values.
Analyze data and build models using the programming language R and Python.
Consider the efficient way to process data on Hadoop.

The dataset and readme canbe found here:
https://www.ncdc.noaa.gov/data-access/land-based-station-data/land-based-datasets/global-historical-climatology-network-ghcn
